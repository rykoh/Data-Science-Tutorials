{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/web-scraping-5649074f3ead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a web scraper using Python and BeautifulSoup (a library) to scrape data of FIFA World Cup 2018. The data includes an individual player’s information and statistics of the whole world cup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 — Understanding What Data to Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 — Loading the Web page in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Request URL\n",
    "page = requests.get(\"https://www.fifa.com/worldcup/players.html\")\n",
    "\n",
    "#Fetch webpage\n",
    "soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests: The first thing we are going to need to scrape the page is to download the page. We are going to do that with Python’s requests library.\n",
    "# BeautifulSoup: We will use this library to parse the HTML page we’ve just downloaded. In other words, we’ll extract the data we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWith requests.get you first get the webpage by passing the URL. Now, we create an instance of BeautifulSoup. We will print that instance to check whether the web page loaded correctly or not. Adding .prettify() makes code cleaner so that it is readable to humans. As you can see below, our page has loaded correctly. Now we can scrape data from it.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "With requests.get you first get the webpage by passing the URL. Now, we create an instance of BeautifulSoup. We will print that instance to check whether the web page loaded correctly or not. Adding .prettify() makes code cleaner so that it is readable to humans. As you can see below, our page has loaded correctly. Now we can scrape data from it.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 — First Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ronaldo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Request URL\n",
    "page = requests.get(\"https://www.fifa.com/worldcup/players/player/201200/profile.html\")\n",
    "\n",
    "#Fetch webpage\n",
    "soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "\n",
    "#Scraping Data\n",
    "name = soup.find(\"div\",{\"class\":\"fi-p__name\"})\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe have requested the page where Ronaldo’s profile resides. Created an instance. Now, with find method of BeautifulSoup, we will find what we need. We need div but there are a lot of divs in the code, which one do you need specifically? We need the one where the class name is fi-p__name only. Because in that division, the name of the player is written. Hitting enter and voila, we got it.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We have requested the page where Ronaldo’s profile resides. Created an instance. Now, with find method of BeautifulSoup, we will find what we need. We need div but there are a lot of divs in the code, which one do you need specifically? We need the one where the class name is fi-p__name only. Because in that division, the name of the player is written. Hitting enter and voila, we got it.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplifying the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Request URL\n",
    "page = requests.get(\"https://www.fifa.com/worldcup/players/player/201200/profile.html\")\n",
    "\n",
    "#Fetch webpage\n",
    "soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "\n",
    "#Scraping Data\n",
    "name = soup.find(\"div\",{\"class\":\"fi-p__name\"}).text.replace(\"\\n\",\"\").strip()\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can’t store the data in the format we got because of obvious reasons. We need it in the text format. That’s why we have added .text at the end. And this is what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still not cool, we have several “\\n” there. We got to remove them as well. So we will replace them with nothing (which is “”). And we will have something like this,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One more thing, removing preceding and following spaces, for that, .strip() and the final output looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There we have it. We have fetched the name of one player. With the same thing, we will fetch other details which you saw on the web page. Like, height, country, role, and goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 — All Data of First Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Request URL\n",
    "page = requests.get(\"https://www.fifa.com/worldcup/players/player/201200/profile.html\")\n",
    "\n",
    "#Fetch webpage\n",
    "soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "\n",
    "#Scraping Data\n",
    "#Name #Country #Role #Age #Height #International Caps #International Goals\n",
    "player_name = soup.find(\"div\",{\"class\":\"fi-p__name\"}).text.replace(\"\\n\",\"\").strip()\n",
    "player_country = soup.find(\"div\",{\"class\":\"fi-p__country\"}).text.replace(\"\\n\",\"\").strip()\n",
    "player_role = soup.find(\"div\",{\"class\":\"fi-p__role\"}).text.replace(\"\\n\",\"\").strip()\n",
    "player_age = soup.find(\"div\",{\"class\":\"fi-p__profile-number__number\"}).text.replace(\"\\n\",\"\").strip()\n",
    "player_height = soup.find_all(\"div\",{\"class\":\"fi-p__profile-number__number\"})[1].text.replace(\"\\n\",\"\").strip()\n",
    "player_int_caps = soup.find_all(\"div\",{\"class\":\"fi-p__profile-number__number\"})[2].text.replace(\"\\n\",\"\").strip()\n",
    "player_int_goals = soup.find_all(\"div\",{\"class\":\"fi-p__profile-number__number\"})[3].text.replace(\"\\n\",\"\").strip()\n",
    "\n",
    "print(player_name,\"\\n\",player_country,\"\\n\",player_role,\"\\n\",player_age,\"years \\n\",player_height,\"\\n\",player_int_caps,\"caps \\n\",player_int_goals,\"goals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each parameter has its own class. Easy for us. While height, caps, and goals don’t, huh. No worries. We will use find_all method and then we will print each with the index number in their order. Tada, we made it simple too. The output looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 — All Data of All 736 Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThat’s the data of one player we got, how about others? All player’s profiles are on different web pages. We have to scrape it all with just one script and not individual scripts for each player. What are we going to do now? Find a pattern, so that we can fetch all URL’s at the same time.\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "That’s the data of one player we got, how about others? All player’s profiles are on different web pages. We have to scrape it all with just one script and not individual scripts for each player. What are we going to do now? Find a pattern, so that we can fetch all URL’s at the same time.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAs you can see, the URL of the Ronaldo’s profile is, https://fifa.com/worldcup/players/player/201200/profile.html, do you see something which might be common for all? Yes, correct, there is. The player id (for Ronaldo — 201200) might be unique for all of them. So, before scraping all other data, we have to scrape player IDs. Just like we scraped the name, but this time we will run a loop.\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "As you can see, the URL of the Ronaldo’s profile is, https://fifa.com/worldcup/players/player/201200/profile.html, do you see something which might be common for all? Yes, correct, there is. The player id (for Ronaldo — 201200) might be unique for all of them. So, before scraping all other data, we have to scrape player IDs. Just like we scraped the name, but this time we will run a loop.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Player List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas\n",
    "\n",
    "#Empty list to store data\n",
    "id_list = []\n",
    "\n",
    "#Fetching URL\n",
    "request = requests.get(\"https://www.fifa.com/worldcup/players/_libraries/byposition/[id]/_players-list\")\n",
    "soup = BeautifulSoup(request.content,\"html.parser\")\n",
    "\n",
    "#Iterate to find all IDs\n",
    "for ids in range(0,736):\n",
    "    all = soup.find_all(\"a\",\"fi-p--link\")[ids]\n",
    "    id_list.append(all['data-player-id'])\n",
    "\n",
    "#Data Frame to store scrapped data\n",
    "df = pandas.DataFrame({\n",
    "\"Ids\":id_list\n",
    "})\n",
    "df.to_csv('player_ids.csv', index = False)\n",
    "print(df,\"\\n Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nI checked the website, there was a total of 736 players, hence, running a loop to for the same. Here, in place of div there is a , the anchor tag. But the functionality remains the same — Searching and finding our class. We are creating an empty list at the beginning, and then appending the IDs to that list in each iteration. In the end, we will create a data frame to store those IDs and export them to .CSV file.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "I checked the website, there was a total of 736 players, hence, running a loop to for the same. Here, in place of div there is a , the anchor tag. But the functionality remains the same — Searching and finding our class. We are creating an empty list at the beginning, and then appending the IDs to that list in each iteration. In the end, we will create a data frame to store those IDs and export them to .CSV file.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Fetching All Data of All Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the IDs, and know how to fetch each parameter, we just have to pass those IDs and run all through the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas\n",
    "from collections import OrderedDict\n",
    "\n",
    "#Fetch data of Player's ID\n",
    "player_ids = pandas.read_csv(\"player_ids.csv\")\n",
    "ids = player_ids[\"Ids\"]\n",
    "\n",
    "#Prive a base url and an empty list\n",
    "base_url = \"https://www.fifa.com/worldcup/players/player/\"\n",
    "player_list = []\n",
    "\n",
    "#Iterate to scrap data of players from fifa.com\n",
    "for pages in ids:\n",
    "    #Using OrderedDict instead of Dict (See explaination)\n",
    "    d=OrderedDict()\n",
    "    #Fetching URLs one by one\n",
    "    print(base_url+str(pages)+\"/profile.html\")\n",
    "    request = requests.get(base_url+str(pages)+\"/profile.html\")\n",
    "    #Data processing\n",
    "    content = request.content\n",
    "    soup = BeautifulSoup(content,\"html.parser\")\n",
    "    #Scraping Data\n",
    "    #Name #Country #Role #Age #Height #International Caps #International Goals\n",
    "    d['Name'] = soup.find(\"div\",{\"class\":\"fi-p__name\"}).text.replace(\"\\n\",\"\").strip()\n",
    "    print(d['Name'])\n",
    "    d['Country'] = soup.find(\"div\",{\"class\":\"fi-p__country\"}).text.replace(\"\\n\",\"\").strip()\n",
    "    d['Role'] = soup.find(\"div\",{\"class\":\"fi-p__role\"}).text.replace(\"\\n\",\"\").strip()\n",
    "    d['Age'] = soup.find(\"div\",{\"class\":\"fi-p__profile-number__number\"}).text.replace(\"\\n\",\"\").strip()\n",
    "    d['Height(cm)'] = soup.find_all(\"div\",{\"class\":\"fi-p__profile-number__number\"})[1].text.replace(\"\\n\",\"\").strip()\n",
    "    d['International Caps'] = soup.find_all(\"div\",{\"class\":\"fi-p__profile-number__number\"})[2].text.replace(\"\\n\",\"\").strip()\n",
    "    d['International Goals'] = soup.find_all(\"div\",{\"class\":\"fi-p__profile-number__number\"})[3].text.replace(\"\\n\",\"\").strip()\n",
    "\n",
    "#Append dictionary to list\n",
    "player_list.append(d)\n",
    "#Create a pandas DataFrame to store data and save it to .csv\n",
    "df = pandas.DataFrame(player_list)\n",
    "df.to_csv('Players_info.csv', index = False)\n",
    "print(\"Success \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe have created a base URL, so when we iterate through each ID, only ID in the URL will change, and we will get data from all the URLs. One more thing is, rather than lists, we are using dictionaries because there are multiple parameters. And specifically Ordered Dictionary because we want the same order as we fetch the data and not the sorted one. Append a dictionary to list and create a data frame. Save the Data to CSV and there you have it. I’ve printed out only the name of the player while running the script because that way I can be sure that data is fetched correctly.\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We have created a base URL, so when we iterate through each ID, only ID in the URL will change, and we will get data from all the URLs. One more thing is, rather than lists, we are using dictionaries because there are multiple parameters. And specifically Ordered Dictionary because we want the same order as we fetch the data and not the sorted one. Append a dictionary to list and create a data frame. Save the Data to CSV and there you have it. I’ve printed out only the name of the player while running the script because that way I can be sure that data is fetched correctly.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/borrkk/web-scraped-fifa-worldcup-2018-data-from-fifacom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Dhrumilcse/Web-Scrapper-FIFA.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
