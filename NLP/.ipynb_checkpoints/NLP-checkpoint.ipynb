{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/how-to-use-nlp-in-python-a-practical-step-by-step-example-bd82ca2d2e1e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation: Scraping the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We scrape the job postings for “data scientists” from Indeed for 8 different cities. Upon scraping, we download the data into separate files for each of the cities.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''We scrape the job postings for “data scientists” from Indeed for 8 different cities. Upon scraping, we download the data into separate files for each of the cities.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 8 cities included in this analysis are Boston, Chicago, Los Angeles, Montreal, New York, San Francisco, Toronto, and Vancouver. The variables are job_title, company, location, and job_description.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''The 8 cities included in this analysis are Boston, Chicago, Los Angeles, Montreal, New York, San Francisco, Toronto, and Vancouver. The variables are job_title, company, location, and job_description.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/liannewriting/a08e549f186067837856494513250ff1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import ElementNotVisibleException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# define scraping function\n",
    "def scrape_indeed(search,loc, limit = 50, canada=False):\n",
    "    \n",
    "    # search_term is the keyword/designation to be searched\n",
    "    search_term = search.replace(' ','+')                  \n",
    "    \n",
    "    if canada:\n",
    "        url = 'https://www.indeed.ca/jobs?q={}&l={}&limit={}&radius=25&start=0'.format(search_term, loc, limit)\n",
    "    else:\n",
    "        url = 'https://www.indeed.com/jobs?q={}&l={}&limit={}&radius=25&start=0'.format(search_term, loc, limit)\n",
    "    \n",
    "    # Start the browser and load the above URL\n",
    "    browser = webdriver.Chrome('/Users/justin/Downloads/chromedriver')\n",
    "    browser.get(url)\n",
    "    \n",
    "    # Empty dataframe in which we will store our data scraped from job posts\n",
    "    data = pd.DataFrame(columns = ['job_title','company', 'location', 'job_description'])\n",
    "\n",
    "    x = 0\n",
    "    \n",
    "    # get the number of results. This determines\n",
    "    num_results = browser.find_element_by_id('searchCountPages').text\n",
    "    ind0 = num_results.find('of ') + 3\n",
    "    ind1 = num_results.find(' ', ind0)\n",
    "    num_results = int(num_results[ind0:ind1])\n",
    "    pages = math.ceil(num_results/limit) # the number of pages to visit.\n",
    "    \n",
    "    # Loop through the pages\n",
    "    for j in range(pages):\n",
    "        \n",
    "        # All the job posts have class 'row result clickcard'.\n",
    "        job_elements =  browser.find_elements_by_xpath(\"//div[@class='jobsearch-SerpJobCard unifiedRow row result clickcard']\")\n",
    "\n",
    "        # Loop through the individual job posts\n",
    "        for i in range(len(job_elements)):\n",
    "            \n",
    "            # Click on the job post\n",
    "            job_elements[i].click()\n",
    "            \n",
    "            # Sleep for minimum 3 seconds because we dont want to create unnecessary load on Indeed's servers\n",
    "            sleep(3 + random.randint(0,3))\n",
    "            \n",
    "            # Sometimes Selenium might start scraping before the page finishes loading or \n",
    "            # we might encounter '404 : Job not found error'\n",
    "            # Although these occurences are very rare we don't want our job scrapper to crash.\n",
    "            # Therefore we will retry before moving on.\n",
    "            # If the data was successfully scrapped then it will break out of the for loop\n",
    "            # If we encounter error it will retry again provided the retry count is below 5\n",
    "            \n",
    "            done = False\n",
    "            for k in range(0,5):\n",
    "                try:\n",
    "                    title =  browser.find_element_by_id('vjs-jobtitle').text\n",
    "                    company = browser.find_element_by_id('vjs-cn').text\n",
    "                    company = company.replace('- ', '')\n",
    "                    \n",
    "                    location = browser.find_element_by_id('vjs-loc').text\n",
    "                    description = browser.find_element_by_id('vjs-desc').text\n",
    "                    done = True\n",
    "                    break\n",
    "                except NoSuchElementException:\n",
    "                    print('Unable to fetch data. Retrying.....')\n",
    "\n",
    "            if not done:\n",
    "                continue\n",
    "\n",
    "            # For debugging purposes lets log the job post scrapped\n",
    "            print('Completed Post {} of Page {} - {}'.format(i+1,j+1,title))\n",
    "            \n",
    "            # Insert the data into our dataframe\n",
    "            data = data.append({'job_title':title,\n",
    "                                'company':company,\n",
    "                                'location':location,\n",
    "                                'job_description':description},ignore_index=True)    \n",
    "            \n",
    "\n",
    "        # Change the URL, so as to move on to the next page\n",
    "        url = url.replace('start=' + str(x),'start=' +str(x+limit))\n",
    "        x += limit\n",
    "        \n",
    "        if len(job_elements) < limit:\n",
    "            break\n",
    "        \n",
    "        browser.get(url)\n",
    "        print('Moving on to page ' + str(j+2))\n",
    "        sleep(2)\n",
    "        \n",
    "        # A popover appears when we go to the next page. We will tell the browser to click on close button.\n",
    "        # Although so far for me it has appeared only on 2nd page but I have included the check for every page to be on safer side\n",
    "        try:\n",
    "            browser.find_element_by_id('popover-x').click()\n",
    "        except:\n",
    "            print('No Newsletter Popup Found')\n",
    "    \n",
    "    browser.close()\n",
    "    return data\n",
    "\n",
    "# download data, use Toronto as an example\n",
    "loc = 'Toronto%2C+ON'\n",
    "q = 'title%3A%28machine+learning%29'\n",
    "\n",
    "df0 = scrape_indeed(q, loc, 50, True) # Jan 25\n",
    "df0.to_pickle('data_scientist_toronto.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Loading and Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we load and combine the data files of the 8 cities into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data.\n",
    "df_list = []\n",
    "cities = ['boston', 'chicago', 'la', 'montreal', 'ny', 'sf', 'toronto', 'vancouver']\n",
    "\n",
    "for city in cities:\n",
    "    df_tmp = pd.read_pickle('data_scientist_{}.pkl'.format(city))\n",
    "    df_tmp['city'] = city\n",
    "    df_list.append(df_tmp)\n",
    "\n",
    "df = pd.concat(df_list).reset_index(drop=True)\n",
    "\n",
    "# make the city names nicer.\n",
    "msk = df['city'] == 'la'\n",
    "df.loc[msk, 'city'] = 'los angeles'\n",
    "\n",
    "msk = df['city'] == 'ny'\n",
    "df.loc[msk, 'city'] = 'new york'\n",
    "\n",
    "msk = df['city'] == 'sf'\n",
    "df.loc[msk, 'city'] = 'san francisco'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove duplicate rows/job postings with the same job_title, job_description, and city features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it's the same job description in the same city, for the same job title, we consider it duplicate.\n",
    "print(df.shape)\n",
    "df = df.drop_duplicates(subset=['job_description', 'city', 'job_title'])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step #2: Forming the Lists of Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Before searching in the job descriptions, we need lists of keywords that represent the tools/skills/degrees.\\nFor this analysis, we use a simple approach to forming the lists. The lists are based on our judgment and the content of the job postings. You may use more advanced approaches if the task is more complicated than this.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Before searching in the job descriptions, we need lists of keywords that represent the tools/skills/degrees.\n",
    "For this analysis, we use a simple approach to forming the lists. The lists are based on our judgment and the content of the job postings. You may use more advanced approaches if the task is more complicated than this.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://monkeylearn.com/keyword-extraction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For the list of keywords of tools, we initially come up with a list based on our knowledge of data science. We know that the popular tools for data scientists include Python, R, Hadoop, Spark, and more. We have a decent knowledge of the field. So this initial list is good to have covered many tools mentioned in the job postings.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''For the list of keywords of tools, we initially come up with a list based on our knowledge of data science. We know that the popular tools for data scientists include Python, R, Hadoop, Spark, and more. We have a decent knowledge of the field. So this initial list is good to have covered many tools mentioned in the job postings.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Then we look at random job postings and add tools that are not on the list yet. Often these new keywords remind us to add other related tools as well.\\nAfter this process, we have a keyword list that covers most of the tools mentioned in the job postings.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Then we look at random job postings and add tools that are not on the list yet. Often these new keywords remind us to add other related tools as well.\n",
    "After this process, we have a keyword list that covers most of the tools mentioned in the job postings.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Next, we separate the keywords into a single-word list and a multi-word list. We need to match these two lists of keywords to the job description in different ways.\\nWith simple string matches, the multi-word keyword is often unique and easy to identify in the job description.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Next, we separate the keywords into a single-word list and a multi-word list. We need to match these two lists of keywords to the job description in different ways.\n",
    "With simple string matches, the multi-word keyword is often unique and easy to identify in the job description.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The single-word keyword, such as “c” is referring to C programming language in our article. But “c” is also a common letter that is used in many words including “can”, “clustering”. We need to process them further (through tokenization) to match only when there is a single letter “c” in the job descriptions.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''The single-word keyword, such as “c” is referring to C programming language in our article. But “c” is also a common letter that is used in many words including “can”, “clustering”. We need to process them further (through tokenization) to match only when there is a single letter “c” in the job descriptions.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are our lists of keywords for tools coded in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# got these keywords by looking at some examples and using existing knowledge.\n",
    "tool_keywords1 = ['python', 'pytorch', 'sql', 'mxnet', 'mlflow', 'einstein', 'theano', 'pyspark', 'solr', 'mahout', \n",
    " 'cassandra', 'aws', 'powerpoint', 'spark', 'pig', 'sas', 'java', 'nosql', 'docker', 'salesforce', 'scala', 'r',\n",
    " 'c', 'c++', 'net', 'tableau', 'pandas', 'scikitlearn', 'sklearn', 'matlab', 'scala', 'keras', 'tensorflow', 'clojure',\n",
    " 'caffe', 'scipy', 'numpy', 'matplotlib', 'vba', 'spss', 'linux', 'azure', 'cloud', 'gcp', 'mongodb', 'mysql', 'oracle', \n",
    " 'redshift', 'snowflake', 'kafka', 'javascript', 'qlik', 'jupyter', 'perl', 'bigquery', 'unix', 'react',\n",
    " 'scikit', 'powerbi', 's3', 'ec2', 'lambda', 'ssrs', 'kubernetes', 'hana', 'spacy', 'tf', 'django', 'sagemaker',\n",
    " 'seaborn', 'mllib', 'github', 'git', 'elasticsearch', 'splunk', 'airflow', 'looker', 'rapidminer', 'birt', 'pentaho', \n",
    " 'jquery', 'nodejs', 'd3', 'plotly', 'bokeh', 'xgboost', 'rstudio', 'shiny', 'dash', 'h20', 'h2o', 'hadoop', 'mapreduce', \n",
    " 'hive', 'cognos', 'angular', 'nltk', 'flask', 'node', 'firebase', 'bigtable', 'rust', 'php', 'cntk', 'lightgbm', \n",
    " 'kubeflow', 'rpython', 'unixlinux', 'postgressql', 'postgresql', 'postgres', 'hbase', 'dask', 'ruby', 'julia', 'tensor',\n",
    "# added r packages doesn't seem to impact the result\n",
    " 'dplyr','ggplot2','esquisse','bioconductor','shiny','lubridate','knitr','mlr','quanteda','dt','rcrawler','caret','rmarkdown',\n",
    " 'leaflet','janitor','ggvis','plotly','rcharts','rbokeh','broom','stringr','magrittr','slidify','rvest',\n",
    " 'rmysql','rsqlite','prophet','glmnet','text2vec','snowballc','quantmod','rstan','swirl','datasciencer']\n",
    "\n",
    "\n",
    "# another set of keywords that are longer than one word.\n",
    "tool_keywords2 = set(['amazon web services', 'google cloud', 'sql server'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get lists of keywords for skills by following a similar process as tools.\n",
    "skill_keywords1 = set(['statistics', 'cleansing', 'chatbot', 'cleaning', 'blockchain', 'causality', 'correlation', 'bandit', 'anomaly', 'kpi',\n",
    " 'dashboard', 'geospatial', 'ocr', 'econometrics', 'pca', 'gis', 'svm', 'svd', 'tuning', 'hyperparameter', 'hypothesis',\n",
    " 'salesforcecom', 'segmentation', 'biostatistics', 'unsupervised', 'supervised', 'exploratory',\n",
    " 'recommender', 'recommendations', 'research', 'sequencing', 'probability', 'reinforcement', 'graph', 'bioinformatics',\n",
    " 'chi', 'knn', 'outlier', 'etl', 'normalization', 'classification', 'optimizing', 'prediction', 'forecasting',\n",
    " 'clustering', 'cluster', 'optimization', 'visualization', 'nlp', 'c#',\n",
    " 'regression', 'logistic', 'nn', 'cnn', 'glm',\n",
    " 'rnn', 'lstm', 'gbm', 'boosting', 'recurrent', 'convolutional', 'bayesian',\n",
    " 'bayes'])\n",
    "\n",
    "\n",
    "# another set of keywords that are longer than one word.\n",
    "skill_keywords2 = set(['random forest', 'natural language processing', 'machine learning', 'decision tree', 'deep learning', 'experimental design',\n",
    " 'time series', 'nearest neighbors', 'neural network', 'support vector machine', 'computer vision', 'machine vision', 'dimensionality reduction', \n",
    " 'text analytics', 'power bi', 'a/b testing', 'ab testing', 'chat bot', 'data mining'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For education level, we use a different procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Because we are looking for the minimum required education level, we need a numeric value to rank the education degree. For example, we use 1 to represent “bachelor” or “undergraduate”, 2 to represent “master” or “graduate”, and so on.\\nIn this way, we have a ranking of degrees by numbers from 1 to 4. The higher the number, the higher the education level.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Because we are looking for the minimum required education level, we need a numeric value to rank the education degree. For example, we use 1 to represent “bachelor” or “undergraduate”, 2 to represent “master” or “graduate”, and so on.\n",
    "In this way, we have a ranking of degrees by numbers from 1 to 4. The higher the number, the higher the education level.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_dict = {'bs': 1, 'bachelor': 1, 'undergraduate': 1, \n",
    "               'master': 2, 'graduate': 2, 'mba': 2.5, \n",
    "               'phd': 3, 'ph.d': 3, 'ba': 1, 'ma': 2,\n",
    "               'postdoctoral': 4, 'postdoc': 4, 'doctorate': 3}\n",
    "\n",
    "\n",
    "degree_dict2 = {'advanced degree': 2, 'ms or': 2, 'ms degree': 2, '4 year degree': 1, 'bs/': 1, 'ba/': 1,\n",
    "                '4-year degree': 1, 'b.s.': 1, 'm.s.': 2, 'm.s': 2, 'b.s': 1, 'phd/': 3, 'ph.d.': 3, 'ms/': 2,\n",
    "                'm.s/': 2, 'm.s./': 2, 'msc/': 2, 'master/': 2, 'master\\'s/': 2, 'bachelor\\s/': 1}\n",
    "degree_keywords2 = set(degree_dict2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step #3: Streamlining the Job Descriptions using NLP Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In this step, we streamline the job description text. We make the text easier to understand by computer programs; and hence more efficient to match the text with the lists of keywords.\\nThe job_description feature in our dataset looks like this.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''In this step, we streamline the job description text. We make the text easier to understand by computer programs; and hence more efficient to match the text with the lists of keywords.\n",
    "The job_description feature in our dataset looks like this.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['job_description'].iloc[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the Job Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tokenization is a process of parsing the text string into different sections (tokens). It is necessary since the computer programs understand the tokenized text better.\\nWe must explicitly split the job description text string into different tokens (words) with delimiters such as space (“ ”). We use the word_tokenize function to handle this task.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Tokenization is a process of parsing the text string into different sections (tokens). It is necessary since the computer programs understand the tokenized text better.\n",
    "We must explicitly split the job description text string into different tokens (words) with delimiters such as space (“ ”). We use the word_tokenize function to handle this task.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(df['job_description'].iloc[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After this process, the job description text string is partitioned into tokens (words) as below. The computer can read and process these tokens easier.\\nFor instance, the single-word keyword “c” can only match with tokens (words) “c”, rather than with other words “can” or “clustering”.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''After this process, the job description text string is partitioned into tokens (words) as below. The computer can read and process these tokens easier.\n",
    "For instance, the single-word keyword “c” can only match with tokens (words) “c”, rather than with other words “can” or “clustering”.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parts of Speech (POS) Tagging the Job Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The job descriptions are often long. We want to keep the words that are informative for our analysis while filtering out others. We use POS tagging to achieve this.\\nThe POS tagging is an NLP method of labeling whether a word is a noun, adjective, verb, etc. Wikipedia explains it well:\\nPOS tagging is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context — i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.\\nThanks to the NLTK, we can use this tagger with Python.\\nApplying this technique on the lists of keywords, we can find tags related to our analysis.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''The job descriptions are often long. We want to keep the words that are informative for our analysis while filtering out others. We use POS tagging to achieve this.\n",
    "The POS tagging is an NLP method of labeling whether a word is a noun, adjective, verb, etc. Wikipedia explains it well:\n",
    "POS tagging is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context — i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.\n",
    "Thanks to the NLTK, we can use this tagger with Python.\n",
    "Applying this technique on the lists of keywords, we can find tags related to our analysis.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below, we POS tag the list of keywords for tools as a demonstration.\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "pos_tag(tool_keywords1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Different combinations of letters represent the tags. For instance, NN stands for nouns and singular words such as “python”, JJ stands for adjective words such as “big”. The full list of representations is here.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Different combinations of letters represent the tags. For instance, NN stands for nouns and singular words such as “python”, JJ stands for adjective words such as “big”. The full list of representations is here.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see, the tagger is not perfect. For example, “sql” is tagged as “JJ” — adjective. But it is still good enough to help us filtering for useful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We use this list of tags of all the keywords as a filter for the job descriptions. We keep only the words from the job descriptions that have these same tags of keywords. For example, we would keep the words from job descriptions with tags “NN” and “JJ”. By doing this, we filter out the words from the job descriptions such as “the”, “then” that are not informative for our analysis.\\nAt this stage, we have streamlined job descriptions that are tokenized and shortened.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''We use this list of tags of all the keywords as a filter for the job descriptions. We keep only the words from the job descriptions that have these same tags of keywords. For example, we would keep the words from job descriptions with tags “NN” and “JJ”. By doing this, we filter out the words from the job descriptions such as “the”, “then” that are not informative for our analysis.\n",
    "At this stage, we have streamlined job descriptions that are tokenized and shortened.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step #4: Final Processing of the Keywords and the Job Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this step, we process both the lists of keywords and the job descriptions further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming the Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Word stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base, or root form — generally a written word form.\\nThe stemming process allows computer programs to identify the words of the same stem despite their different look. In this way, we can match words as long as they have the same stem. For instance, the words “models”, “modeling” both have the same stem of “model”.\\nWe stem both the lists of keywords and the streamlined job descriptions.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Word stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base, or root form — generally a written word form.\n",
    "The stemming process allows computer programs to identify the words of the same stem despite their different look. In this way, we can match words as long as they have the same stem. For instance, the words “models”, “modeling” both have the same stem of “model”.\n",
    "We stem both the lists of keywords and the streamlined job descriptions.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing the Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lastly, we standardize all the words by lowercasing them. We only lowercase the job descriptions since the lists of keywords are built in lowercase.\\nAs mentioned in the previous sections, the Python code used in the previous procedures is below.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Lastly, we standardize all the words by lowercasing them. We only lowercase the job descriptions since the lists of keywords are built in lowercase.\n",
    "As mentioned in the previous sections, the Python code used in the previous procedures is below.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "# process the job description.\n",
    "def prepare_job_desc(desc):\n",
    "    # tokenize description.\n",
    "    tokens = word_tokenize(desc)\n",
    "        \n",
    "    # Parts of speech (POS) tag tokens.\n",
    "    token_tag = pos_tag(tokens)\n",
    "    \n",
    "    # Only include some of the POS tags.\n",
    "    include_tags = ['VBN', 'VBD', 'JJ', 'JJS', 'JJR', 'CD', 'NN', 'NNS', 'NNP', 'NNPS']\n",
    "    filtered_tokens = [tok for tok, tag in token_tag if tag in include_tags]\n",
    "    \n",
    "    # stem words.\n",
    "    stemmed_tokens = [ps.stem(tok).lower() for tok in filtered_tokens]\n",
    "    return set(stemmed_tokens)\n",
    "\n",
    "df['job_description_word_set'] = df['job_description'].map(prepare_job_desc)\n",
    "\n",
    "# process the keywords\n",
    "tool_keywords1_set = set([ps.stem(tok) for tok in tool_keywords1]) # stem the keywords (since the job description is also stemmed.)\n",
    "tool_keywords1_dict = {ps.stem(tok):tok for tok in tool_keywords1} # use this dictionary to revert the stemmed words back to the original.\n",
    "\n",
    "skill_keywords1_set = set([ps.stem(tok) for tok in skill_keywords1])\n",
    "skill_keywords1_dict = {ps.stem(tok):tok for tok in skill_keywords1}\n",
    "\n",
    "degree_keywords1_set = set([ps.stem(tok) for tok in degree_dict.keys()])\n",
    "degree_keywords1_dict = {ps.stem(tok):tok for tok in degree_dict.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now only the words (tokens) in the job descriptions that are related to our analysis remain. An example of a final job description is below.\n",
    "df['job_description_word_set'].iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step #5: Matching the Keywords and the Job Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see if a job description mentions specific keywords, we match the lists of keywords and the final streamlined job descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools/Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As you may recall, we built two types of keyword lists — the single-word list and the multi-word list. For the single-word keywords, we match each keyword with the job description by the set intersection function. For the multi-word keywords, we check whether they are sub-strings of the job descriptions.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''As you may recall, we built two types of keyword lists — the single-word list and the multi-word list. For the single-word keywords, we match each keyword with the job description by the set intersection function. For the multi-word keywords, we check whether they are sub-strings of the job descriptions.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For the education level, we use the same method as tools/skills to match keywords. Yet, we only keep track of the minimum level.\\nFor example, when the keywords “bachelor” and “master” both exist in a job description, the bachelor’s degree is the minimum education required for this job.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''For the education level, we use the same method as tools/skills to match keywords. Yet, we only keep track of the minimum level.\n",
    "For example, when the keywords “bachelor” and “master” both exist in a job description, the bachelor’s degree is the minimum education required for this job.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_list = []\n",
    "skill_list = []\n",
    "degree_list = []\n",
    "\n",
    "msk = df['city'] != '' # just in case you want to filter the data.\n",
    "\n",
    "num_postings = len(df[msk].index)\n",
    "for i in range(num_postings):\n",
    "    job_desc = df[msk].iloc[i]['job_description'].lower()\n",
    "    job_desc_set = df[msk].iloc[i]['job_description_word_set']\n",
    "    \n",
    "    # check if the keywords are in the job description. Look for exact match by token.\n",
    "    tool_words = tool_keywords1_set.intersection(job_desc_set)\n",
    "    skill_words = skill_keywords1_set.intersection(job_desc_set)\n",
    "    degree_words = degree_keywords1_set.intersection(job_desc_set)\n",
    "    \n",
    "    # check if longer keywords (more than one word) are in the job description. Match by substring.\n",
    "    j = 0\n",
    "    for tool_keyword2 in tool_keywords2:\n",
    "        # tool keywords.\n",
    "        if tool_keyword2 in job_desc:\n",
    "            tool_list.append(tool_keyword2)\n",
    "            j += 1\n",
    "    \n",
    "    k = 0\n",
    "    for skill_keyword2 in skill_keywords2:\n",
    "        # skill keywords.\n",
    "        if skill_keyword2 in job_desc:\n",
    "            skill_list.append(skill_keyword2)\n",
    "            k += 1\n",
    "    \n",
    "    # search for the minimum education.\n",
    "    min_education_level = 999\n",
    "    for degree_word in degree_words:\n",
    "        level = degree_dict[degree_keywords1_dict[degree_word]]\n",
    "        min_education_level = min(min_education_level, level)\n",
    "    \n",
    "    for degree_keyword2 in degree_keywords2:\n",
    "        # longer keywords. Match by substring.\n",
    "        if degree_keyword2 in job_desc:\n",
    "            level = degree_dict2[degree_keyword2]\n",
    "            min_education_level = min(min_education_level, level)\n",
    "    \n",
    "    # label the job descriptions without any tool keywords.\n",
    "    if len(tool_words) == 0 and j == 0:\n",
    "        tool_list.append('nothing specified')\n",
    "    \n",
    "    # label the job descriptions without any skill keywords.\n",
    "    if len(skill_words) == 0 and k == 0:\n",
    "        skill_list.append('nothing specified')\n",
    "    \n",
    "    # If none of the keywords were found, but the word degree is present, then assume it's a bachelors level.\n",
    "    if min_education_level > 500:\n",
    "        if 'degree' in job_desc:\n",
    "            min_education_level = 1\n",
    "    \n",
    "    tool_list += list(tool_words)\n",
    "    skill_list += list(skill_words)\n",
    "    degree_list.append(min_education_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Visualizing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We summarize the results with bar charts.\\nFor each particular keyword of tools/skills/education levels, we count the number of job descriptions that match them. We calculate their percentage among all the job descriptions as well.\\nFor the lists of tools and skills, we are only presenting the top 50 most popular ones. For the education level, we summarize them according to the minimum level required.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''We summarize the results with bar charts.\n",
    "For each particular keyword of tools/skills/education levels, we count the number of job descriptions that match them. We calculate their percentage among all the job descriptions as well.\n",
    "For the lists of tools and skills, we are only presenting the top 50 most popular ones. For the education level, we summarize them according to the minimum level required.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top Tools In-Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the list of tools.\n",
    "df_tool = pd.DataFrame(data={'cnt': tool_list})\n",
    "df_tool = df_tool.replace(tool_keywords1_dict)\n",
    "\n",
    "# group some of the categories together.\n",
    "msk = df_tool['cnt'] == 'h20'\n",
    "df_tool.loc[msk, 'cnt'] = 'h2o'\n",
    "\n",
    "msk = df_tool['cnt'] == 'aws'\n",
    "df_tool.loc[msk, 'cnt'] = 'amazon web services'\n",
    "\n",
    "msk = df_tool['cnt'] == 'gcp'\n",
    "df_tool.loc[msk, 'cnt'] = 'google cloud'\n",
    "\n",
    "msk = df_tool['cnt'] == 'github'\n",
    "df_tool.loc[msk, 'cnt'] = 'git'\n",
    "\n",
    "msk = df_tool['cnt'] == 'postgressql'\n",
    "df_tool.loc[msk, 'cnt'] = 'postgres'\n",
    "\n",
    "msk = df_tool['cnt'] == 'tensor'\n",
    "df_tool.loc[msk, 'cnt'] = 'tensorflow'\n",
    "\n",
    "df_tool_top50 = df_tool['cnt'].value_counts().reset_index().rename(columns={'index': 'tool'}).iloc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the tools.\n",
    "layout = dict(\n",
    "    title='Tools For Data Scientists',\n",
    "    yaxis=dict(\n",
    "        title='% of job postings',\n",
    "        tickformat=',.0%',\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(layout=layout)\n",
    "fig.add_trace(go.Bar(\n",
    "    x=df_tool_top50['tool'],\n",
    "    y=df_tool_top50['cnt']/num_postings\n",
    "))\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top Skills In-Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the list of skills/knowledge.\n",
    "df_skills = pd.DataFrame(data={'cnt': skill_list})\n",
    "df_skills = df_skills.replace(skill_keywords1_dict)\n",
    "\n",
    "# group some of the categories together.\n",
    "msk = df_skills['cnt'] == 'nlp'\n",
    "df_skills.loc[msk, 'cnt'] = 'natural language processing'\n",
    "\n",
    "msk = df_skills['cnt'] == 'convolutional'\n",
    "df_skills.loc[msk, 'cnt'] = 'convolutional neural network'\n",
    "\n",
    "msk = df_skills['cnt'] == 'cnn'\n",
    "df_skills.loc[msk, 'cnt'] = 'convolutional neural network'\n",
    "\n",
    "msk = df_skills['cnt'] == 'recurrent'\n",
    "df_skills.loc[msk, 'cnt'] = 'recurrent neural network'\n",
    "\n",
    "msk = df_skills['cnt'] == 'rnn'\n",
    "df_skills.loc[msk, 'cnt'] = 'recurrent neural network'\n",
    "\n",
    "msk = df_skills['cnt'] == 'knn'\n",
    "df_skills.loc[msk, 'cnt'] = 'nearest neighbors'\n",
    "\n",
    "msk = df_skills['cnt'] == 'svm'\n",
    "df_skills.loc[msk, 'cnt'] = 'support vector machine'\n",
    "\n",
    "msk = df_skills['cnt'] == 'machine vision'\n",
    "df_skills.loc[msk, 'cnt'] = 'computer vision'\n",
    "\n",
    "msk = df_skills['cnt'] == 'ab testing'\n",
    "df_skills.loc[msk, 'cnt'] = 'a/b testing'\n",
    "\n",
    "df_skills_top50 = df_skills['cnt'].value_counts().reset_index().rename(columns={'index': 'skill'}).iloc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the skills.\n",
    "layout = dict(\n",
    "    title='Skills For Data Scientists',\n",
    "    yaxis=dict(\n",
    "        title='% of job postings',\n",
    "        tickformat=',.0%',\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(layout=layout)\n",
    "fig.add_trace(go.Bar(\n",
    "    x=df_skills_top50['skill'],\n",
    "    y=df_skills_top50['cnt']/num_postings\n",
    "))\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum Education Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the list of degree.\n",
    "df_degrees = pd.DataFrame(data={'cnt': degree_list})\n",
    "df_degrees['degree_type'] = ''\n",
    "\n",
    "\n",
    "msk = df_degrees['cnt'] == 1\n",
    "df_degrees.loc[msk, 'degree_type'] = 'bachelors'\n",
    "\n",
    "msk = df_degrees['cnt'] == 2\n",
    "df_degrees.loc[msk, 'degree_type'] = 'masters'\n",
    "\n",
    "msk = df_degrees['cnt'] == 3\n",
    "df_degrees.loc[msk, 'degree_type'] = 'phd'\n",
    "\n",
    "msk = df_degrees['cnt'] == 4\n",
    "df_degrees.loc[msk, 'degree_type'] = 'postdoc'\n",
    "\n",
    "msk = df_degrees['cnt'] == 2.5\n",
    "df_degrees.loc[msk, 'degree_type'] = 'mba'\n",
    "\n",
    "msk = df_degrees['cnt'] > 500\n",
    "df_degrees.loc[msk, 'degree_type'] = 'not specified'\n",
    "\n",
    "\n",
    "df_degree_cnt = df_degrees['degree_type'].value_counts().reset_index().rename(columns={'index': 'degree'}).iloc[:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the degrees.\n",
    "layout = dict(\n",
    "    title='Minimum Education For Data Scientists',\n",
    "    yaxis=dict(\n",
    "        title='% of job postings',\n",
    "        tickformat=',.0%',\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(layout=layout)\n",
    "fig.add_trace(go.Bar(\n",
    "    x=df_degree_cnt['degree'],\n",
    "    y=df_degree_cnt['degree_type']/num_postings\n",
    "))\n",
    "\n",
    "iplot(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
